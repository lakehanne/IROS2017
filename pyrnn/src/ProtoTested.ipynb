{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch [1/500], Loss: nan\n",
      "Epoch [11/500], Loss: nan\n",
      "Epoch [21/500], Loss: nan\n",
      "Epoch [31/500], Loss: nan\n",
      "Epoch [41/500], Loss: nan\n",
      "Epoch [51/500], Loss: nan\n",
      "Epoch [61/500], Loss: nan\n",
      "Epoch [71/500], Loss: nan\n",
      "Epoch [81/500], Loss: nan\n",
      "Epoch [91/500], Loss: nan\n",
      "Epoch [101/500], Loss: nan\n",
      "Epoch [111/500], Loss: nan\n",
      "Epoch [121/500], Loss: nan\n",
      "Epoch [131/500], Loss: nan\n",
      "Epoch [141/500], Loss: nan\n",
      "Epoch [151/500], Loss: nan\n",
      "Epoch [161/500], Loss: nan\n",
      "Epoch [171/500], Loss: nan\n",
      "Epoch [181/500], Loss: nan\n",
      "Epoch [191/500], Loss: nan\n",
      "Epoch [201/500], Loss: nan\n",
      "Epoch [211/500], Loss: nan\n",
      "Epoch [221/500], Loss: nan\n",
      "Epoch [231/500], Loss: nan\n",
      "Epoch [241/500], Loss: nan\n",
      "Epoch [251/500], Loss: nan\n",
      "Epoch [261/500], Loss: nan\n",
      "Epoch [271/500], Loss: nan\n",
      "Epoch [281/500], Loss: nan\n",
      "Epoch [291/500], Loss: nan\n",
      "Epoch [301/500], Loss: nan\n",
      "Epoch [311/500], Loss: nan\n",
      "Epoch [321/500], Loss: nan\n",
      "Epoch [331/500], Loss: nan\n",
      "Epoch [341/500], Loss: nan\n",
      "Epoch [351/500], Loss: nan\n",
      "Epoch [361/500], Loss: nan\n",
      "Epoch [371/500], Loss: nan\n",
      "Epoch [381/500], Loss: nan\n",
      "Epoch [391/500], Loss: nan\n",
      "Epoch [401/500], Loss: nan\n",
      "Epoch [411/500], Loss: nan\n",
      "Epoch [421/500], Loss: nan\n",
      "Epoch [431/500], Loss: nan\n",
      "Epoch [441/500], Loss: nan\n",
      "Epoch [451/500], Loss: nan\n",
      "Epoch [461/500], Loss: nan\n",
      "Epoch [471/500], Loss: nan\n",
      "Epoch [481/500], Loss: nan\n",
      "Epoch [491/500], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    '''\n",
    "    nn.LSTM Parameters:\n",
    "        input_size  – The number of expected features in the input x\n",
    "        hidden_size – The number of features in the hidden state h\n",
    "        num_layers  – Number of recurrent layers.\n",
    "\n",
    "    Inputs: input, (h_0, c_0)\n",
    "        input (seq_len, batch, input_size)\n",
    "        h_0 (num_layers * num_directions, batch, hidden_size)\n",
    "        c_0 (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "    Outputs: output, (h_n, c_n)\n",
    "        output (seq_len, batch, hidden_size * num_directions)\n",
    "        h_n (num_layers * num_directions, batch, hidden_size)\n",
    "        c_n (num_layers * num_directions, batch, hidden_size):\n",
    "    '''\n",
    "    def __init__(self, inputSize, nHidden, batchSize, noutputs=3, numLayers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.cost = nn.MSELoss(size_average=False)\n",
    "        self.noutputs = noutputs\n",
    "        self.num_layers = numLayers\n",
    "        self.inputSize = inputSize\n",
    "        self.nHidden = nHidden\n",
    "        self.batchSize = batchSize\n",
    "        self.noutputs = noutputs\n",
    "        \n",
    "        #define recurrent and linear layers\n",
    "        self.lstm1  = nn.LSTM(inputSize,nHidden[0],num_layers=numLayers)\n",
    "        self.lstm2  = nn.LSTM(nHidden[0],nHidden[1],num_layers=numLayers)\n",
    "        self.lstm3  = nn.LSTM(nHidden[1],nHidden[2],num_layers=numLayers)\n",
    "        self.fc     = nn.Linear(nHidden[2], noutputs)\n",
    "        self.drop   = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[0])) \n",
    "        c0 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[0]))        \n",
    "        # Forward propagate RNN layer 1\n",
    "        out, _ = self.lstm1(x, (h0, c0)) \n",
    "        out = self.drop(out)\n",
    "        \n",
    "        # Set hidden layer 2 states \n",
    "        h1 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[1])) \n",
    "        c1 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[1]))        \n",
    "        # Forward propagate RNN layer 2\n",
    "        out, _ = self.lstm2(out, (h1, c1))  \n",
    "        \n",
    "        # Set hidden layer 3 states \n",
    "        h2 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[2])) \n",
    "        c2 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[2]))        \n",
    "        # Forward propagate RNN layer 2\n",
    "        out, _ = self.lstm3(out, (h2, c2)) \n",
    "        out = self.drop(out) \n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "#hyperparams\n",
    "inputSize = 9\n",
    "nHidden   = [9,6,6]\n",
    "numLayers = 2\n",
    "sequence_length = 9\n",
    "num_epochs = 500\n",
    "noutputs = 3\n",
    "batchSize = 1\n",
    "\n",
    "lstm = LSTMModel(inputSize, nHidden, batchSize, noutputs, numLayers)\n",
    "#lstm.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=0.1)\n",
    "\n",
    "#inputs and outputs\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = Variable(torch.Tensor(5, 1, 9))\n",
    "    labels = Variable(torch.Tensor(5, 3))\n",
    "\n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm(inputs)\n",
    "    loss    = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (4) : unspecified launch failure at /home/robotec/Documents/NNs/locuclab-pytorch/torch/lib/THC/generic/THCTensorCopy.c:18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-919b3e4d10bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/robotec/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device_id, async)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/robotec/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/tensor.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/robotec/anaconda2/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (4) : unspecified launch failure at /home/robotec/Documents/NNs/locuclab-pytorch/torch/lib/THC/generic/THCTensorCopy.c:18"
     ]
    }
   ],
   "source": [
    "inputs.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named rospkg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-f7418f972a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrospy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeometry_msgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeometry_msgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ros/indigo/lib/python2.7/dist-packages/rospy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstd_msgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyargv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mget_published_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mwait_for_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ros/indigo/lib/python2.7/dist-packages/rospy/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrosgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mroslib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrospy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ros/indigo/lib/python2.7/dist-packages/roslib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.7.0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mroslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_manifest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# this import is necessary due to a bug in purge_build.py in our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ros/indigo/lib/python2.7/dist-packages/roslib/launcher.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrospkg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# bootstrapped keeps track of which packages we've loaded so we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named rospkg"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function insert:\n",
      "\n",
      "insert(...)\n",
      "    L.insert(index, object) -- insert object before index\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "help(sys.path.insert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = Variable(torch.Tensor(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 5.5608e+31\n",
       " 4.5883e-41\n",
       " 1.9190e-09\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 5.5608e+31\n",
       " 4.5883e-41\n",
       " 1.9190e-09\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
