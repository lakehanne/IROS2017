{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 21515037492637815799808.0000\n",
      "Epoch [11/500], Loss: nan\n",
      "Epoch [21/500], Loss: nan\n",
      "Epoch [31/500], Loss: nan\n",
      "Epoch [41/500], Loss: nan\n",
      "Epoch [51/500], Loss: nan\n",
      "Epoch [61/500], Loss: nan\n",
      "Epoch [71/500], Loss: nan\n",
      "Epoch [81/500], Loss: nan\n",
      "Epoch [91/500], Loss: nan\n",
      "Epoch [101/500], Loss: nan\n",
      "Epoch [111/500], Loss: nan\n",
      "Epoch [121/500], Loss: nan\n",
      "Epoch [131/500], Loss: nan\n",
      "Epoch [141/500], Loss: nan\n",
      "Epoch [151/500], Loss: nan\n",
      "Epoch [161/500], Loss: nan\n",
      "Epoch [171/500], Loss: nan\n",
      "Epoch [181/500], Loss: nan\n",
      "Epoch [191/500], Loss: nan\n",
      "Epoch [201/500], Loss: nan\n",
      "Epoch [211/500], Loss: nan\n",
      "Epoch [221/500], Loss: nan\n",
      "Epoch [231/500], Loss: nan\n",
      "Epoch [241/500], Loss: nan\n",
      "Epoch [251/500], Loss: nan\n",
      "Epoch [261/500], Loss: nan\n",
      "Epoch [271/500], Loss: nan\n",
      "Epoch [281/500], Loss: nan\n",
      "Epoch [291/500], Loss: nan\n",
      "Epoch [301/500], Loss: nan\n",
      "Epoch [311/500], Loss: nan\n",
      "Epoch [321/500], Loss: nan\n",
      "Epoch [331/500], Loss: nan\n",
      "Epoch [341/500], Loss: nan\n",
      "Epoch [351/500], Loss: nan\n",
      "Epoch [361/500], Loss: nan\n",
      "Epoch [371/500], Loss: nan\n",
      "Epoch [381/500], Loss: nan\n",
      "Epoch [391/500], Loss: nan\n",
      "Epoch [401/500], Loss: nan\n",
      "Epoch [411/500], Loss: nan\n",
      "Epoch [421/500], Loss: nan\n",
      "Epoch [431/500], Loss: nan\n",
      "Epoch [441/500], Loss: nan\n",
      "Epoch [451/500], Loss: nan\n",
      "Epoch [461/500], Loss: nan\n",
      "Epoch [471/500], Loss: nan\n",
      "Epoch [481/500], Loss: nan\n",
      "Epoch [491/500], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    '''\n",
    "    nn.LSTM Parameters:\n",
    "        input_size  – The number of expected features in the input x\n",
    "        hidden_size – The number of features in the hidden state h\n",
    "        num_layers  – Number of recurrent layers.\n",
    "\n",
    "    Inputs: input, (h_0, c_0)\n",
    "        input (seq_len, batch, input_size)\n",
    "        h_0 (num_layers * num_directions, batch, hidden_size)\n",
    "        c_0 (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "    Outputs: output, (h_n, c_n)\n",
    "        output (seq_len, batch, hidden_size * num_directions)\n",
    "        h_n (num_layers * num_directions, batch, hidden_size)\n",
    "        c_n (num_layers * num_directions, batch, hidden_size):\n",
    "    '''\n",
    "    def __init__(self, inputSize, nHidden, batchSize, noutputs=3, numLayers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.cost = nn.MSELoss(size_average=False)\n",
    "        self.noutputs = noutputs\n",
    "        self.num_layers = numLayers\n",
    "        self.inputSize = inputSize\n",
    "        self.nHidden = nHidden\n",
    "        self.batchSize = batchSize\n",
    "        self.noutputs = noutputs\n",
    "        \n",
    "        #define recurrent and linear layers\n",
    "        self.lstm1  = nn.LSTM(inputSize,nHidden[0],num_layers=numLayers)\n",
    "        self.lstm2  = nn.LSTM(nHidden[0],nHidden[1],num_layers=numLayers)\n",
    "        self.lstm3  = nn.LSTM(nHidden[1],nHidden[2],num_layers=numLayers)\n",
    "        self.fc     = nn.Linear(nHidden[2], noutputs)\n",
    "        self.drop   = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[0])) \n",
    "        c0 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[0]))        \n",
    "        # Forward propagate RNN layer 1\n",
    "        out, _ = self.lstm1(x, (h0, c0)) \n",
    "        out = self.drop(out)\n",
    "        \n",
    "        # Set hidden layer 2 states \n",
    "        h1 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[1])) \n",
    "        c1 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[1]))        \n",
    "        # Forward propagate RNN layer 2\n",
    "        out, _ = self.lstm2(out, (h1, c1))  \n",
    "        \n",
    "        # Set hidden layer 3 states \n",
    "        h2 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[2])) \n",
    "        c2 = Variable(torch.Tensor(self.num_layers, batchSize, self.nHidden[2]))        \n",
    "        # Forward propagate RNN layer 2\n",
    "        out, _ = self.lstm3(out, (h2, c2)) \n",
    "        out = self.drop(out) \n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "#hyperparams\n",
    "inputSize = 9\n",
    "nHidden   = [9,6,6]\n",
    "numLayers = 2\n",
    "sequence_length = 9\n",
    "num_epochs = 500\n",
    "noutputs = 3\n",
    "batchSize = 1\n",
    "\n",
    "lstm = LSTMModel(inputSize, nHidden, batchSize, noutputs, numLayers)\n",
    "#lstm.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=0.1)\n",
    "\n",
    "#inputs and outputs\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = Variable(torch.Tensor(5, 1, 9))\n",
    "    labels = Variable(torch.Tensor(5, 3))\n",
    "\n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm(inputs)\n",
    "    loss    = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "LSTMModel (\n",
      "  (cost): MSELoss (\n",
      "  )\n",
      "  (lstm1): LSTM(9, 9, num_layers=2)\n",
      "  (lstm2): LSTM(9, 6, num_layers=2)\n",
      "  (lstm3): LSTM(6, 6, num_layers=2)\n",
      "  (fc): Linear (6 -> 3)\n",
      "  (drop): Dropout (p = 0.3)\n",
      ")\n",
      "Epoch [1/100], Loss: nan\n",
      "Epoch [11/100], Loss: nan\n",
      "Epoch [21/100], Loss: nan\n",
      "Epoch [31/100], Loss: nan\n",
      "Epoch [41/100], Loss: nan\n",
      "Epoch [51/100], Loss: nan\n",
      "Epoch [61/100], Loss: nan\n",
      "Epoch [71/100], Loss: nan\n",
      "Epoch [81/100], Loss: nan\n",
      "Epoch [91/100], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import absolute_import\n",
    "\n",
    "# coding=utf-8\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import torch.cuda\n",
    "import numpy as np\n",
    "\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    '''\n",
    "    nn.LSTM Parameters:\n",
    "        input_size  – The number of expected features in the input x\n",
    "        hidden_size – The number of features in the hidden state h\n",
    "        num_layers  – Number of recurrent layers.\n",
    "\n",
    "    Inputs: input, (h_0, c_0)\n",
    "        input (seq_len, batch, input_size)\n",
    "        h_0 (num_layers * num_directions, batch, hidden_size)\n",
    "        c_0 (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "    Outputs: output, (h_n, c_n)\n",
    "        output (seq_len, batch, hidden_size * num_directions)\n",
    "        h_n (num_layers * num_directions, batch, hidden_size)\n",
    "        c_n (num_layers * num_directions, batch, hidden_size):\n",
    "\n",
    "    QP Layer:\n",
    "        nz = 6, neq = 0, nineq = 12, QPenalty = 0.1\n",
    "    '''\n",
    "    def __init__(self, nz, neq, nineq, Qpenalty, inputSize, nHidden, batchSize, noutputs=3, numLayers=2):\n",
    "\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # QP Parameters\n",
    "        nx = nz * 2  #cause inequality is double sided (see my notes)\n",
    "        self.neq = neq\n",
    "        self.nineq = ineq\n",
    "        self.nz = nz\n",
    "\n",
    "        self.Q = Variable(Qpenalty*torch.eye(nx).double())\n",
    "        self.G = Variable(torch.eye(nx).double())\n",
    "        for i in range(6):\n",
    "            self.G[i][i] *= -1\n",
    "        self.h = Variable(torch.ones(nx).double())\n",
    "        self.A = 1000*npr.randn(neq, nz)\n",
    "        self.b = Variable(torch.zeros(self.A.size(0)).double())\n",
    "        def qp_layer(x):\n",
    "            nBatch = x.size(0)\n",
    "            Q = self.Q.unsqueeze(0).expand(nBatch, self.nz, nHidden)\n",
    "            G = self.G.unsqueeze(0).expand(nBatch, nHidden, self.nHidden)\n",
    "            h = self.h.unsqueeze(0).expand(nBatch, self.nineq)\n",
    "            A = self.A.unsqueeze(0).expand(nBatch, 1, self.neq)\n",
    "            b = self.b.unsqueeze(0).expand(nBatch, 1)\n",
    "            x = QPFunction()(x.double(), Q, G, h, A, b).float()\n",
    "            return x\n",
    "        self.qp_layer = qp_layer\n",
    "\n",
    "\n",
    "        # Backprop Through Time (Recurrent Layer) Params\n",
    "        self.cost = nn.MSELoss(size_average=False)\n",
    "        self.noutputs = noutputs\n",
    "        self.num_layers = numLayers\n",
    "        self.inputSize = inputSize\n",
    "        self.nHidden = nHidden\n",
    "        self.batchSize = batchSize\n",
    "        self.noutputs = noutputs\n",
    "        self.criterion = nn.MSELoss(size_average=False)\n",
    "        \n",
    "        #define recurrent and linear layers\n",
    "        self.lstm1  = nn.LSTM(inputSize,nHidden[0],num_layers=numLayers)\n",
    "        self.lstm2  = nn.LSTM(nHidden[0],nHidden[1],num_layers=numLayers)\n",
    "        self.lstm3  = nn.LSTM(nHidden[1],nHidden[2],num_layers=numLayers)\n",
    "        self.fc     = nn.Linear(nHidden[2], noutputs)\n",
    "        self.drop   = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        nBatch = x.size(0)\n",
    "\n",
    "        # Set initial states \n",
    "        h0 = Variable(torch.Tensor(self.num_layers, self.batchSize, self.nHidden[0])) \n",
    "        c0 = Variable(torch.Tensor(self.num_layers, self.batchSize, self.nHidden[0]))        \n",
    "        # Forward propagate RNN layer 1\n",
    "        out, _ = self.lstm1(x, (h0, c0)) \n",
    "        out = self.drop(out)\n",
    "        \n",
    "        # Set hidden layer 2 states \n",
    "        h1 = Variable(torch.Tensor(self.num_layers, self.batchSize, self.nHidden[1])) \n",
    "        c1 = Variable(torch.Tensor(self.num_layers, self.batchSize, self.nHidden[1]))        \n",
    "        # Forward propagate RNN layer 2\n",
    "        out, _ = self.lstm2(out, (h1, c1))  \n",
    "        \n",
    "        # Set hidden layer 3 states \n",
    "        h2 = Variable(torch.Tensor(self.num_layers, self.batchSize, self.nHidden[2])) \n",
    "        c2 = Variable(torch.Tensor(self.num_layers, self.batchSize, self.nHidden[2]))        \n",
    "        # Forward propagate RNN layer 2\n",
    "        out, _ = self.lstm3(out, (h2, c2)) \n",
    "        out = self.drop(out) \n",
    "        \n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "\n",
    "        #Now add QP Layer\n",
    "        out = out.view(nBatch, -1) \n",
    "        return self.qp_layer(out)\n",
    "    \n",
    "#hyperparams\n",
    "inputSize = 9\n",
    "nHidden   = [9,6,6]\n",
    "numLayers = 2\n",
    "sequence_length = 9\n",
    "num_epochs = 100\n",
    "noutputs = 3\n",
    "batchSize = 1\n",
    "\n",
    "lstm = LSTMModel(inputSize, nHidden, batchSize, noutputs, numLayers)\n",
    "\n",
    "print lstm\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=0.1)\n",
    "\n",
    "#inputs and outputs\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = Variable(torch.Tensor(5, 1, 9))\n",
    "    labels = Variable(torch.Tensor(5, 3))\n",
    "\n",
    "    # Forward + Backward + Optimize\n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm(inputs)\n",
    "    loss    = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch % 10) == 0:\n",
    "        print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "if mode is None:\n",
    "    print('True')\n",
    "else:\n",
    "    print('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
